{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load inputs from last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.load(\"intermediate_values/final_embedding.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5668, -0.1368, -2.1955,  0.1668, -1.2070, -0.3723, -0.8302, -2.8953,\n",
      "          1.8269, -0.3231],\n",
      "        [ 1.3479,  0.5237, -0.7732, -0.4390, -1.2601, -0.9128,  1.7754,  1.2303,\n",
      "         -4.0808,  0.9466],\n",
      "        [ 1.5389, -1.7982,  2.1728,  0.5161,  1.5061,  0.9178, -0.7344, -0.9022,\n",
      "         -0.5057,  1.5436],\n",
      "        [-0.2512, -0.5905, -1.9205, -1.4082,  2.2990,  0.5391, -0.9468, -1.6840,\n",
      "          0.3417,  1.7622],\n",
      "        [-0.6245,  0.1875,  2.7057,  0.8237, -0.5156, -2.3120, -0.3651,  1.9557,\n",
      "          0.2960, -0.3758],\n",
      "        [ 0.3872,  2.9064,  2.4349, -0.0070, -1.0729, -2.1992,  2.6361,  0.7379,\n",
      "          2.5921, -0.6817],\n",
      "        [ 1.0945, -0.7756,  1.9551, -1.8756, -0.6402,  0.8235,  1.5774,  1.5828,\n",
      "         -0.5951, -1.5694]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input[0,:,:])\n",
    "input.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer Norm 1\n",
    "- Normalise features within a layer (not across batch)\n",
    "- Mean and variance are calculated for all activations in a layer\n",
    "- These are scaled and shifted to have standard normal distribution (mean=0; variance=1)\n",
    "- Handles problems of Internal covariate shift\n",
    "\n",
    "#### Terminologies\n",
    "1. Activations = Outputs of the neurons\n",
    "2. Internal covariate shift = During training as each layer takes inputs from previous layers and the input distribution keeps changing as each layer is learning. This leads to slow converges\n",
    "\n",
    "#### Formula\n",
    "### output = [scale * (input - mean)/ sqrt(variance + epsilon)] + shift\n",
    "* mean = mean across activations\n",
    "* variance = (Std_dev)^2 = Variance across activations\n",
    "* epsilon = small constant to avoid division by zero\n",
    "* scale, shift = learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7533],\n",
      "         [-0.1642],\n",
      "         [ 0.4255],\n",
      "         [-0.1859],\n",
      "         [ 0.1776],\n",
      "         [ 0.7734],\n",
      "         [ 0.1578]],\n",
      "\n",
      "        [[-0.3153],\n",
      "         [ 0.2092],\n",
      "         [ 0.5860],\n",
      "         [-0.3911],\n",
      "         [ 0.9981],\n",
      "         [ 0.5299],\n",
      "         [ 0.0224]],\n",
      "\n",
      "        [[-0.6828],\n",
      "         [ 1.1236],\n",
      "         [ 1.0298],\n",
      "         [ 0.0039],\n",
      "         [ 0.1424],\n",
      "         [ 0.1010],\n",
      "         [ 1.1030]]], grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = input.mean(dim=-1, keepdim=True)  # Dim=-1 => Along Embedding size (which is learnt in previous layers)\n",
    "print(mean)\n",
    "mean.shape  # batch_size * in_seq_len * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5756],\n",
      "         [2.7248],\n",
      "         [1.5881],\n",
      "         [1.8268],\n",
      "         [1.7907],\n",
      "         [2.9210],\n",
      "         [1.7790]],\n",
      "\n",
      "        [[0.6766],\n",
      "         [2.1250],\n",
      "         [2.2524],\n",
      "         [2.0671],\n",
      "         [0.5906],\n",
      "         [1.5930],\n",
      "         [4.0861]],\n",
      "\n",
      "        [[1.5324],\n",
      "         [2.5206],\n",
      "         [2.1610],\n",
      "         [1.6786],\n",
      "         [1.5481],\n",
      "         [3.2254],\n",
      "         [2.8710]]], grad_fn=<VarBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = input.var(dim=-1, keepdim=True, unbiased=False)  # Turning off Unbiased avoids division by zero \n",
    "print(var)\n",
    "var.shape # batch_size * in_seq_len * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6481,  0.4912, -1.1489,  0.7330, -0.3614,  0.3036, -0.0613, -1.7064,\n",
      "          2.0556,  0.3427],\n",
      "        [ 0.9161,  0.4168, -0.3689, -0.1665, -0.6639, -0.4535,  1.1750,  0.8448,\n",
      "         -2.3727,  0.6729],\n",
      "        [ 0.8835, -1.7645,  1.3866,  0.0719,  0.8575,  0.3907, -0.9204, -1.0536,\n",
      "         -0.7389,  0.8872],\n",
      "        [-0.0483, -0.2993, -1.2834, -0.9043,  1.8385,  0.5364, -0.5630, -1.1084,\n",
      "          0.3904,  1.4414],\n",
      "        [-0.5994,  0.0074,  1.8893,  0.4828, -0.5180, -1.8604, -0.4055,  1.3288,\n",
      "          0.0885, -0.4135],\n",
      "        [-0.2260,  1.2481,  0.9722, -0.4566, -1.0803, -1.7393,  1.0899, -0.0208,\n",
      "          1.0641, -0.8514],\n",
      "        [ 0.7024, -0.6998,  1.3476, -1.5245, -0.5982,  0.4991,  1.0644,  1.0684,\n",
      "         -0.5645, -1.2949]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1e-5\n",
    "norm_input = (input - mean)/torch.sqrt(var + epsilon)\n",
    "print(norm_input[0,:,:])\n",
    "norm_input.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = nn.Parameter(torch.ones(emd_dim))\n",
    "print(scale)\n",
    "scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift = nn.Parameter(torch.zeros(emd_dim))\n",
    "print(shift)\n",
    "shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6481,  0.4912, -1.1489,  0.7330, -0.3614,  0.3036, -0.0613, -1.7064,\n",
      "          2.0556,  0.3427],\n",
      "        [ 0.9161,  0.4168, -0.3689, -0.1665, -0.6639, -0.4535,  1.1750,  0.8448,\n",
      "         -2.3727,  0.6729],\n",
      "        [ 0.8835, -1.7645,  1.3866,  0.0719,  0.8575,  0.3907, -0.9204, -1.0536,\n",
      "         -0.7389,  0.8872],\n",
      "        [-0.0483, -0.2993, -1.2834, -0.9043,  1.8385,  0.5364, -0.5630, -1.1084,\n",
      "          0.3904,  1.4414],\n",
      "        [-0.5994,  0.0074,  1.8893,  0.4828, -0.5180, -1.8604, -0.4055,  1.3288,\n",
      "          0.0885, -0.4135],\n",
      "        [-0.2260,  1.2481,  0.9722, -0.4566, -1.0803, -1.7393,  1.0899, -0.0208,\n",
      "          1.0641, -0.8514],\n",
      "        [ 0.7024, -0.6998,  1.3476, -1.5245, -0.5982,  0.4991,  1.0644,  1.0684,\n",
      "         -0.5645, -1.2949]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_norm_input = (scale * norm_input) + shift\n",
    "print(final_norm_input[0,:,:])\n",
    "final_norm_input.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save to carry forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_norm_input,\"intermediate_values/layer_norm_1_output.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
