{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Batch Creation with tokenised texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Inputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "in_seq_len = 7 # All inputs to be of same length = Either truncated from left or padded with eot_tokens to the left\n",
    "# The in sequence tokens *cannot* be greater than context length supported by the model\n",
    "\n",
    "# Samples\n",
    "txt1 = \"Every effort moves you towards your goal\"  # Text with token_len=in_seq_len\n",
    "txt2 = \"Every day holds a\"  # Text with token_len<in_seq_len\n",
    "txt3 = \"This statement is going to have token length greater than input sequence length\"  # Text with token_len>in_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Tokenize each text in batch & convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_tensor_length(tensor, in_seq_len, eot_token):\n",
    "    if tensor.size(0) > in_seq_len:\n",
    "        # Truncate from the left\n",
    "        return tensor[-in_seq_len:]\n",
    "    elif tensor.size(0) < in_seq_len:\n",
    "        # Pad with eot_token to the left\n",
    "        padding = torch.full((in_seq_len - tensor.size(0),), eot_token, dtype=tensor.dtype, device=tensor.device)\n",
    "        return torch.cat((padding, tensor), dim=0)\n",
    "    else:\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust size to in_seq_length\n",
    "\n",
    "txt1_tokens = adjust_tensor_length(torch.tensor(tokenizer.encode(txt1)), in_seq_len, tokenizer.eot_token)\n",
    "txt2_tokens = adjust_tensor_length(torch.tensor(tokenizer.encode(txt2)), in_seq_len, tokenizer.eot_token)\n",
    "txt3_tokens = adjust_tensor_length(torch.tensor(tokenizer.encode(txt3)), in_seq_len, tokenizer.eot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6109, 3626, 6100,  345, 3371,  534, 3061])\n",
      "tensor([50256, 50256, 50256,  6109,  1110,  6622,   257])\n",
      "tensor([11241,  4129,  3744,   621,  5128,  8379,  4129])\n",
      "torch.Size([7]) torch.Size([7]) torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(txt1_tokens)\n",
    "print(txt2_tokens)\n",
    "print(txt3_tokens)\n",
    "print(txt1_tokens.shape, txt2_tokens.shape, txt3_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6109,  3626,  6100,   345,  3371,   534,  3061],\n",
      "        [50256, 50256, 50256,  6109,  1110,  6622,   257],\n",
      "        [11241,  4129,  3744,   621,  5128,  8379,  4129]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 0.3 Create batch\n",
    "batch = []\n",
    "batch.append(txt1_tokens)\n",
    "batch.append(txt2_tokens)\n",
    "batch.append(txt3_tokens)\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "\n",
    "print(batch)\n",
    "batch.shape  # batch_size * in_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257  # Size of gpt2 tokenizer used\n",
    "emb_dim = 10 # Dimension of Embedding to be created; Actual value: 768\n",
    "context_length = 9 # Context Length supported by the model; Actual value: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = nn.Embedding(vocab_size, emb_dim)  # Size of vocab dictionary, Size of output vector\n",
    "tok_embeds = tok_emb(batch)  # Maps each token of each data in input batch to size of output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6974,  0.4461, -0.9584, -0.5125,  0.1477,  0.1206, -1.6799, -1.2684,\n",
      "          0.7520,  0.0756],\n",
      "        [ 0.3967,  1.1326, -1.3573,  0.6575, -0.9811, -0.4606,  0.4231, -0.5453,\n",
      "         -2.4681,  0.2739],\n",
      "        [-0.7516, -0.2530,  1.7825, -0.0568,  0.0073,  0.5140, -0.5162, -0.5955,\n",
      "         -0.8983, -0.0322],\n",
      "        [-2.0613,  0.7856, -1.1759,  0.4869,  1.4556, -0.5829, -1.5793, -0.4895,\n",
      "          1.1080,  0.5295],\n",
      "        [-0.4142, -0.5999,  1.5383,  0.6595, -1.3727, -1.4834, -0.0532,  0.4266,\n",
      "         -0.4942,  0.1028],\n",
      "        [ 0.1817,  0.5926,  1.5677,  0.9269,  0.6115, -1.0457,  0.4409,  1.3449,\n",
      "          1.3554, -0.3465],\n",
      "        [-0.8319,  1.3842,  1.7604, -2.3966, -1.1095,  0.4116, -0.2831,  0.5327,\n",
      "          2.0340, -1.0048]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tok_embeds[0,:,:])\n",
    "tok_embeds.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Postional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensor of positional ids\n",
    "pos_ids = torch.arange(in_seq_len, device=batch.device)\n",
    "\n",
    "pos_emb = nn.Embedding(context_length, emb_dim)  # Size of context length (max supported), Size of output vector\n",
    "pos_embeds = pos_emb(pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8694, -0.5829, -1.2370,  0.6793, -1.3547, -0.4929,  0.8497, -1.6269,\n",
      "          1.0749, -0.3987],\n",
      "        [ 0.9512, -0.6089,  0.5841, -1.0966, -0.2790, -0.4522,  1.3523,  1.7756,\n",
      "         -1.6127,  0.6727],\n",
      "        [ 2.2905, -1.5452,  0.3904,  0.5729,  1.4988,  0.4038, -0.2181, -0.3068,\n",
      "          0.3925,  1.5758],\n",
      "        [ 1.8101, -1.3761, -0.7446, -1.8951,  0.8433,  1.1220,  0.6325, -1.1945,\n",
      "         -0.7663,  1.2327],\n",
      "        [-0.2103,  0.7874,  1.1674,  0.1641,  0.8570, -0.8286, -0.3119,  1.5290,\n",
      "          0.7902, -0.4786],\n",
      "        [ 0.2054,  2.3139,  0.8673, -0.9339, -1.6844, -1.1535,  2.1952, -0.6071,\n",
      "          1.2367, -0.3352],\n",
      "        [ 1.9264, -2.1599,  0.1947,  0.5211,  0.4694,  0.4119,  1.8605,  1.0501,\n",
      "         -2.6292, -0.5645]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pos_embeds)\n",
    "pos_embeds.shape  # in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Embedding\n",
    "= Token Embedding + Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = tok_embeds + pos_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5668, -0.1368, -2.1955,  0.1668, -1.2070, -0.3723, -0.8302, -2.8953,\n",
      "          1.8269, -0.3231],\n",
      "        [ 1.3479,  0.5237, -0.7732, -0.4390, -1.2601, -0.9128,  1.7754,  1.2303,\n",
      "         -4.0808,  0.9466],\n",
      "        [ 1.5389, -1.7982,  2.1728,  0.5161,  1.5061,  0.9178, -0.7344, -0.9022,\n",
      "         -0.5057,  1.5436],\n",
      "        [-0.2512, -0.5905, -1.9205, -1.4082,  2.2990,  0.5391, -0.9468, -1.6840,\n",
      "          0.3417,  1.7622],\n",
      "        [-0.6245,  0.1875,  2.7057,  0.8237, -0.5156, -2.3120, -0.3651,  1.9557,\n",
      "          0.2960, -0.3758],\n",
      "        [ 0.3872,  2.9064,  2.4349, -0.0070, -1.0729, -2.1992,  2.6361,  0.7379,\n",
      "          2.5921, -0.6817],\n",
      "        [ 1.0945, -0.7756,  1.9551, -1.8756, -0.6402,  0.8235,  1.5774,  1.5828,\n",
      "         -0.5951, -1.5694]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_embedding[0,:,:])\n",
    "final_embedding.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.0  # Keeping it zero. Can be tried with smaller prob number\n",
    "drop_emb = nn.Dropout(drop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = drop_emb(final_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embedding.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to carry forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_embedding,\"intermediate_values/final_embedding.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
