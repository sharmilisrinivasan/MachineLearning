{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Batch Creation with tokenised texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Inputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "in_seq_len = 7 # All inputs to be of same length = Either truncated or padded with 0s\n",
    "# The in sequence tokens *cannot* be greater than context length supported by the model\n",
    "\n",
    "# Samples\n",
    "txt1 = \"Every effort moves you towards your goal\"  # Text with token_len=in_seq_len\n",
    "txt2 = \"Every day holds a\"  # Text with token_len<in_seq_len\n",
    "txt3 = \"This statement is going to have token length greater than input sequence length\"  # Text with token_len>in_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Tokenize each text in batch & convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize to in_seq_length\n",
    "\n",
    "txt1_tokens = torch.tensor(tokenizer.encode(txt1)).resize_(in_seq_len)\n",
    "txt2_tokens = torch.tensor(tokenizer.encode(txt2)).resize_(in_seq_len)\n",
    "txt3_tokens = torch.tensor(tokenizer.encode(txt3)).resize_(in_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6109, 3626, 6100,  345, 3371,  534, 3061])\n",
      "tensor([6109, 1110, 6622,  257,    0,    0,    0])\n",
      "tensor([ 1212,  2643,   318,  1016,   284,   423, 11241])\n",
      "torch.Size([7]) torch.Size([7]) torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(txt1_tokens)\n",
    "print(txt2_tokens)\n",
    "print(txt3_tokens)\n",
    "print(txt1_tokens.shape, txt2_tokens.shape, txt3_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6109,  3626,  6100,   345,  3371,   534,  3061],\n",
      "        [ 6109,  1110,  6622,   257,     0,     0,     0],\n",
      "        [ 1212,  2643,   318,  1016,   284,   423, 11241]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 0.3 Create batch\n",
    "batch = []\n",
    "batch.append(txt1_tokens)\n",
    "batch.append(txt2_tokens)\n",
    "batch.append(txt3_tokens)\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "\n",
    "print(batch)\n",
    "batch.shape  # batch_size * in_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257  # Size of gpt2 tokenizer used\n",
    "emb_dim = 10 # Dimension of Embedding to be created; Actual value: 768\n",
    "context_length = 9 # Context Length supported by the model; Actual value: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = nn.Embedding(vocab_size, emb_dim)  # Size of vocab dictionary, Size of output vector\n",
    "tok_embeds = tok_emb(batch)  # Maps each token of each data in input batch to size of output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.0560e-01, -7.2369e-02,  1.6215e+00,  1.2957e+00, -6.7076e-01,\n",
      "         -4.4406e-01,  9.3164e-02, -9.3621e-01,  2.4426e-01,  2.2015e+00],\n",
      "        [ 2.5781e-01, -3.3988e+00, -5.5075e-01, -4.7603e-01, -1.5854e+00,\n",
      "         -1.4190e+00, -5.6897e-01,  6.1440e-01, -1.5085e-03, -3.8581e-01],\n",
      "        [-9.4619e-01,  1.1671e+00,  4.9519e-01, -2.1238e-01,  4.0831e-01,\n",
      "          7.5337e-01,  6.7045e-01,  7.4886e-02, -1.1705e-01, -3.5606e-01],\n",
      "        [ 6.4367e-01, -2.0547e+00,  1.9315e-01, -1.7959e+00,  3.7737e-01,\n",
      "          3.1932e-01,  1.0136e-01, -3.1545e-01, -4.8681e-01,  1.1560e+00],\n",
      "        [ 7.5275e-01,  6.9840e-01,  3.4175e-01, -2.8001e+00,  1.3978e+00,\n",
      "          2.2762e+00,  4.9106e-02,  4.5074e-01, -1.5248e+00,  1.2301e+00],\n",
      "        [-3.1708e-01, -3.5105e-01,  2.4424e-01,  1.1923e-01,  2.0328e-01,\n",
      "         -2.2486e-01, -1.0199e+00,  3.2237e-02, -7.0953e-01, -8.7331e-01],\n",
      "        [-7.7676e-01,  4.4808e-01,  1.8099e-01, -8.8066e-01, -3.3011e-01,\n",
      "          7.4125e-01, -6.2246e-01,  1.9901e+00, -1.3461e+00,  8.3787e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tok_embeds[0,:,:])\n",
    "tok_embeds.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Postional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensor of positional ids\n",
    "pos_ids = torch.arange(in_seq_len, device=batch.device)\n",
    "\n",
    "pos_emb = nn.Embedding(context_length, emb_dim)  # Size of context length (max supported), Size of output vector\n",
    "pos_embeds = pos_emb(pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5175e+00, -3.0224e-01,  1.0007e+00,  9.1725e-01, -6.7031e-01,\n",
      "          5.5924e-01, -7.2874e-01, -3.9477e-01, -6.8363e-01,  2.1758e-01],\n",
      "        [ 2.9607e-01, -1.7877e+00,  5.0068e-02,  1.5456e+00,  6.8654e-01,\n",
      "         -1.3739e-02,  1.4753e-01,  1.3004e+00,  1.2431e+00,  1.1466e+00],\n",
      "        [-1.2794e+00, -1.4152e+00, -4.3518e-02, -1.3565e+00,  4.2320e-02,\n",
      "         -9.3943e-01,  8.3320e-01, -1.0907e+00, -1.8053e-01, -1.7541e+00],\n",
      "        [-6.6483e-01,  1.6275e+00,  1.9352e+00,  8.0503e-01, -6.0275e-01,\n",
      "         -3.7215e-01,  4.6318e-01,  8.2472e-01,  7.5301e-01,  9.8796e-01],\n",
      "        [-3.5740e-01, -7.4040e-01, -4.6665e-01,  3.1377e-05, -5.1613e-01,\n",
      "         -4.8468e-03,  1.7932e+00, -1.1144e+00, -5.2987e-01, -2.6246e-02],\n",
      "        [-1.3561e+00,  1.2757e-01, -7.7255e-01,  1.6031e+00, -1.2485e+00,\n",
      "          1.1571e+00, -4.3108e-01,  5.4125e-01, -6.6783e-02,  1.2242e+00],\n",
      "        [-5.0875e-02,  1.0655e+00,  2.2317e-01,  1.3689e+00, -1.9937e+00,\n",
      "         -1.3690e+00,  1.7098e+00,  3.3852e-01,  1.1813e+00, -7.8739e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pos_embeds)\n",
    "pos_embeds.shape  # in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Embedding\n",
    "= Token Embedding + Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = tok_embeds + pos_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3231, -0.3746,  2.6221,  2.2130, -1.3411,  0.1152, -0.6356, -1.3310,\n",
      "         -0.4394,  2.4190],\n",
      "        [ 0.5539, -5.1864, -0.5007,  1.0695, -0.8989, -1.4327, -0.4214,  1.9148,\n",
      "          1.2416,  0.7608],\n",
      "        [-2.2256, -0.2482,  0.4517, -1.5689,  0.4506, -0.1861,  1.5037, -1.0158,\n",
      "         -0.2976, -2.1102],\n",
      "        [-0.0212, -0.4272,  2.1284, -0.9909, -0.2254, -0.0528,  0.5645,  0.5093,\n",
      "          0.2662,  2.1440],\n",
      "        [ 0.3954, -0.0420, -0.1249, -2.8000,  0.8817,  2.2713,  1.8423, -0.6637,\n",
      "         -2.0547,  1.2039],\n",
      "        [-1.6732, -0.2235, -0.5283,  1.7224, -1.0452,  0.9322, -1.4510,  0.5735,\n",
      "         -0.7763,  0.3509],\n",
      "        [-0.8276,  1.5135,  0.4042,  0.4883, -2.3238, -0.6278,  1.0873,  2.3286,\n",
      "         -0.1649,  0.0505]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_embedding[0,:,:])\n",
    "final_embedding.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.0  # Keeping it zero. Can be tried with smaller prob number\n",
    "drop_emb = nn.Dropout(drop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = drop_emb(final_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 10])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embedding.shape  # batch_size * in_seq_len * emd_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to carry forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_embedding,\"intermediate_values/final_embedding.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
