{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"124M\"\n",
    "models_dir = \"intermediates/gpt2\"\n",
    "sample_file = \"hparams.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Prep Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "sample_url = os.path.join(base_url, model_size, sample_file)\n",
    "sample_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Prep Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(models_dir, model_size)\n",
    "model_dir\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "sample_destination = os.path.join(model_dir, sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Sample Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(sample_url) as response:\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    print(file_size)\n",
    "\n",
    "    # Check if file exists and has the same size\n",
    "    if os.path.exists(sample_destination):\n",
    "            file_size_local = os.path.getsize(sample_destination)\n",
    "            if file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {sample_destination}\")\n",
    "                # return\n",
    "\n",
    "    # Define the block size for reading the file\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    progress_bar_description = os.path.basename(sample_url)  # Extract filename from URL\n",
    "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "            # Open the destination file in binary write mode\n",
    "            with open(sample_destination, \"wb\") as file:\n",
    "                # Read the file in chunks and write to destination\n",
    "                while True:\n",
    "                    chunk = response.read(block_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))  # Update progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Complete Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sys path for imports to work\n",
    "import sys\n",
    "# print(f\"Before: {\"\\n\".join(sys.path)}\")  ## Optional print to check\n",
    "sys.path.append(\"../../../LLMFromScratch\")\n",
    "# print(f\"After: {\"\\n\".join(sys.path)}\")  ## Optional print to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M3_weightloading.gpt_download import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "        source_file_url = os.path.join(base_url, model_size, filename)\n",
    "        destination_file_path = os.path.join(model_dir, filename)\n",
    "        download_file(source_file_url, destination_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load from TensorFlow checkpoint\n",
    "Since OpenAI used TensorFlow, we will have to install and use TensorFlow for loading the weights;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dir)\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "print(tf_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.list_variables(tf_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Load Sample Layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model/h0/attn/c_attn/w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_value = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))  # np.squeeze removes dim with len=1\n",
    "variable_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "variable_name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_number = int(variable_name_parts[0][1:])  # \"0\" of \"h0\"\n",
    "target_dict = params[\"blocks\"][layer_number]  # Pointer to element in the param dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in variable_name_parts[1:-1]:  # Go from 2nd to last but one element in the variable name and create if key does not exist; Move the pointer to new key\n",
    "    target_dict = target_dict.setdefault(key, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_key = variable_name_parts[-1]  # Get last key name and assign value\n",
    "target_dict[last_key] = variable_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Load Sample Non-Layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all variables in checkpoint that do not belong to the layers\n",
    "non_layer_variables = [\n",
    "    var_name for var_name, _ in tf.train.list_variables(tf_ckpt_path)\n",
    "    if not var_name.startswith(\"model/h\")\n",
    "]\n",
    "print(non_layer_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model/ln_f/b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_value = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))  # np.squeeze removes dim with len=1\n",
    "variable_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "variable_name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = params\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name_parts[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_key = variable_name_parts[-1]\n",
    "destination_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict[destination_key] = variable_value  #  Note: \"ln_f\" in variable name is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Load all params using utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sys path for imports to work\n",
    "import sys\n",
    "# print(f\"Before: {\"\\n\".join(sys.path)}\")  ## Optional print to check\n",
    "sys.path.append(\"../../../LLMFromScratch\")\n",
    "# print(f\"After: {\"\\n\".join(sys.path)}\")  ## Optional print to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M3_weightloading.gpt_download import load_gpt2_params_from_tf_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings[\"n_layer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_params[\"wpe\"].shape, complete_params['wte'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test E2E Utility Function to download and load GPT2 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sys path for imports to work\n",
    "import sys\n",
    "# print(f\"Before: {\"\\n\".join(sys.path)}\")  ## Optional print to check\n",
    "sys.path.append(\"../../../LLMFromScratch\")\n",
    "# print(f\"After: {\"\\n\".join(sys.path)}\")  ## Optional print to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M3_weightloading.gpt_download import download_and_load_gpt2_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"124M\"\n",
    "destination_dir = \"intermediates/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: intermediates/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "test_settings, test_params = download_and_load_gpt2_params(model_size, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
