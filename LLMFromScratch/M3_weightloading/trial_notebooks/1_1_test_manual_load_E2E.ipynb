{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sys path for imports to work\n",
    "import sys\n",
    "# print(f\"Before: {\"\\n\".join(sys.path)}\")  ## Optional print to check\n",
    "sys.path.append(\"../../../LLMFromScratch\")\n",
    "# print(f\"After: {\"\\n\".join(sys.path)}\")  ## Optional print to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recreating GPT Download from previous notebook - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M3_weightloading.gpt_download import download_and_load_gpt2_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"124M\"\n",
    "destination_dir = \"intermediates/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: intermediates/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "gpt2_settings, gpt2_params = download_and_load_gpt2_params(model_size, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12},\n",
       " dict_keys(['blocks', 'b', 'g', 'wpe', 'wte']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_settings, gpt2_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load our GPTModel and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load and update config to GPT2-124M using gpt2_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_length': 9,\n",
       " 'drop_rate': 0.0,\n",
       " 'emb_dim': 10,\n",
       " 'n_heads': 2,\n",
       " 'n_layers': 3,\n",
       " 'qvbias': False,\n",
       " 'vocab_size': 50257}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from M1_simple_gpt_model.config import TRIAL_CONFIG_PARAMS\n",
    "TRIAL_CONFIG_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_124M_PARAMS = {\n",
    "    \"context_length\": gpt2_settings[\"n_ctx\"],  # Using from Original GPT2 Model Settings\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"emb_dim\": gpt2_settings[\"n_embd\"],  # Using from Original GPT2 Model Settings\n",
    "    \"n_heads\": gpt2_settings[\"n_head\"],  # Using from Original GPT2 Model Settings\n",
    "    \"n_layers\": gpt2_settings[\"n_layer\"],  # Using from Original GPT2 Model Settings\n",
    "    \"qvbias\": True,  # Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting qkv_bias to True in our implementation, too\n",
    "    \"vocab_size\": gpt2_settings[\"n_vocab\"]  # Using from Original GPT2 Model Settings\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M1_simple_gpt_model.trial_gpt_model import TrialGPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialGPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (linear_out): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_124m_model = TrialGPTModel(GPT2_124M_PARAMS)\n",
    "gpt2_124m_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "['tok_emb.weight', 'pos_emb.weight', 'transformer_blocks.0.norm_1.scale', 'transformer_blocks.0.norm_1.shift', 'transformer_blocks.0.multi_head_attention.mask', 'transformer_blocks.0.multi_head_attention.linear_query.weight', 'transformer_blocks.0.multi_head_attention.linear_query.bias', 'transformer_blocks.0.multi_head_attention.linear_key.weight', 'transformer_blocks.0.multi_head_attention.linear_key.bias', 'transformer_blocks.0.multi_head_attention.linear_value.weight', 'transformer_blocks.0.multi_head_attention.linear_value.bias', 'transformer_blocks.0.multi_head_attention.out_proj_linear.weight', 'transformer_blocks.0.multi_head_attention.out_proj_linear.bias', 'transformer_blocks.0.norm_2.scale', 'transformer_blocks.0.norm_2.shift', 'transformer_blocks.0.feed_forward.layers.0.weight', 'transformer_blocks.0.feed_forward.layers.0.bias', 'transformer_blocks.0.feed_forward.layers.2.weight', 'transformer_blocks.0.feed_forward.layers.2.bias']\n"
     ]
    }
   ],
   "source": [
    "# To check Model Params\n",
    "parameters_dict = gpt2_124m_model.state_dict()\n",
    "print(len(parameters_dict.keys()))\n",
    "print(list(parameters_dict.keys())[:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assign OpenAI weights to the corresponding weight tensors from gpt2_params in our GPTModel instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M3_weightloading.manual_load import ManualWeightLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_weight_loader = ManualWeightLoading(gpt2_124m_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Token and Position Embedding weights with incoming parameters.\n",
      "============ Transformer Block 0 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 0 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 0 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 0 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 0 with weights and biases from incoming params.\n",
      "============ Transformer Block 1 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 1 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 1 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 1 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 1 with weights and biases from incoming params.\n",
      "============ Transformer Block 2 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 2 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 2 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 2 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 2 with weights and biases from incoming params.\n",
      "============ Transformer Block 3 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 3 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 3 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 3 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 3 with weights and biases from incoming params.\n",
      "============ Transformer Block 4 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 4 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 4 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 4 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 4 with weights and biases from incoming params.\n",
      "============ Transformer Block 5 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 5 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 5 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 5 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 5 with weights and biases from incoming params.\n",
      "============ Transformer Block 6 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 6 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 6 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 6 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 6 with weights and biases from incoming params.\n",
      "============ Transformer Block 7 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 7 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 7 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 7 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 7 with weights and biases from incoming params.\n",
      "============ Transformer Block 8 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 8 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 8 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 8 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 8 with weights and biases from incoming params.\n",
      "============ Transformer Block 9 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 9 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 9 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 9 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 9 with weights and biases from incoming params.\n",
      "============ Transformer Block 10 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 10 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 10 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 10 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 10 with weights and biases from incoming params.\n",
      "============ Transformer Block 11 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 11 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 11 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 11 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 11 with weights and biases from incoming params.\n",
      "================================================\n",
      "All Transformer layers updated with incoming parameters.\n",
      "Updated Final Norm with scale and shift from incoming params.\n",
      "Updated Linear Out with weights and biases from incoming params.\n",
      "All weights updated with incoming parameters.\n"
     ]
    }
   ],
   "source": [
    "manual_weight_loader.assign(gpt2_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test assigned GPTModel instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from M0_data.helpers import GPT2Tokenizer\n",
    "from M1_simple_gpt_model.generate import generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialGPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (linear_out): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Test Strings\n",
    "test_strings = [\"Every effort moves you towards your goal\", \"correct the spelling of a the word: whatver\"]\n",
    "\n",
    "# 2. Tokenizer\n",
    "tokenizer = GPT2Tokenizer()\n",
    "\n",
    "# 3. Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt2_124m_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every effort moves you towards your goal!!!!!!!!!!!!!', 'correct the spelling of a the word: whatver!!!!!!!!!!']\n"
     ]
    }
   ],
   "source": [
    "#  1. Tokenize - Convert words to token IDs\n",
    "tokenized_in_strings = tokenizer.tokenize_batch(test_strings, max_in_seq_len=15)\n",
    "\n",
    "#  2. Generate output with the model\n",
    "output_tokens = generate_text(gpt2_124m_model, tokenized_in_strings, max_tokens_to_generate=5, context_size=GPT2_124M_PARAMS[\"context_length\"], print_interims=False)\n",
    "\n",
    " #  3. Detokenize - Convert Token IDs to Words\n",
    "output = tokenizer.detokenize_batch(output_tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "We know that we loaded the model weights correctly because the model can generate coherent text; if we made even a small mistake, the mode would not be able to do that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
