{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Update sys path for imports to work\n",
    "import sys\n",
    "# print(f\"Before: {\"\\n\".join(sys.path)}\")  ## Optional print to check\n",
    "sys.path.append(\"../../../LLMFromScratch\")\n",
    "# print(f\"After: {\"\\n\".join(sys.path)}\")  ## Optional print to check\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharmilisrinivasan/miniconda3/envs/llm_from_scratch_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from M0_data.helpers import GPT2Tokenizer as CUSTOM_GPT2Tokenizer\n",
    "from M1_simple_gpt_model.generate import generate_text\n",
    "from M1_simple_gpt_model.trial_gpt_model import TrialGPTModel\n",
    "from M3_weightloading.gpt_download import download_and_load_gpt2_params\n",
    "from M3_weightloading.manual_load import ManualWeightLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 1000   # For Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"India is also called as \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Reference output from HuggingFace GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[21569,   318,   635,  1444,   355,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['India is also called as \\xa0the \"India of the future\" by the World Bank.\\nThe Indian government has been trying']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', skip_special_tokens=True, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "encoded_input = tokenizer.batch_encode_plus(test_strings, return_tensors='pt', truncation=True, padding=True)\n",
    "print(encoded_input)\n",
    "\n",
    "# Model\n",
    "reference_model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "torch.manual_seed(seed_value)\n",
    "output = reference_model.generate(**encoded_input)\n",
    "\n",
    "# De-Tokenize\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Original GPT2 Model Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 4.1. Load and update config to GPT2-124M using gpt2_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"124M\"\n",
    "destination_dir = \"intermediates/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: intermediates/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: intermediates/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "gpt2_settings, gpt2_params = download_and_load_gpt2_params(model_size, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_124M_PARAMS = {\n",
    "    \"context_length\": gpt2_settings[\"n_ctx\"],  # Using from Original GPT2 Model Settings\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"emb_dim\": gpt2_settings[\"n_embd\"],  # Using from Original GPT2 Model Settings\n",
    "    \"n_heads\": gpt2_settings[\"n_head\"],  # Using from Original GPT2 Model Settings\n",
    "    \"n_layers\": gpt2_settings[\"n_layer\"],  # Using from Original GPT2 Model Settings\n",
    "    \"qvbias\": True,  # Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting qkv_bias to True in our implementation, too\n",
    "    \"vocab_size\": gpt2_settings[\"n_vocab\"]  # Using from Original GPT2 Model Settings\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gpt2_model = TrialGPTModel(GPT2_124M_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Assign OpenAI weights to our GPTModel instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Token and Position Embedding weights with incoming parameters.\n",
      "============ Transformer Block 0 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 0 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 0 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 0 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 0 with weights and biases from incoming params.\n",
      "============ Transformer Block 1 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 1 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 1 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 1 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 1 with weights and biases from incoming params.\n",
      "============ Transformer Block 2 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 2 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 2 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 2 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 2 with weights and biases from incoming params.\n",
      "============ Transformer Block 3 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 3 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 3 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 3 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 3 with weights and biases from incoming params.\n",
      "============ Transformer Block 4 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 4 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 4 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 4 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 4 with weights and biases from incoming params.\n",
      "============ Transformer Block 5 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 5 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 5 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 5 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 5 with weights and biases from incoming params.\n",
      "============ Transformer Block 6 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 6 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 6 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 6 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 6 with weights and biases from incoming params.\n",
      "============ Transformer Block 7 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 7 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 7 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 7 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 7 with weights and biases from incoming params.\n",
      "============ Transformer Block 8 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 8 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 8 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 8 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 8 with weights and biases from incoming params.\n",
      "============ Transformer Block 9 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 9 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 9 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 9 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 9 with weights and biases from incoming params.\n",
      "============ Transformer Block 10 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 10 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 10 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 10 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 10 with weights and biases from incoming params.\n",
      "============ Transformer Block 11 ============\n",
      "Updated Layer Norm layer 1 of Transformer Block 11 with scale and shift from incoming params.\n",
      "Updated MHA of Transformer Block 11 with weights and biases from incoming params.\n",
      "Updated Layer Norm layer 2 of Transformer Block 11 with scale and shift from incoming params.\n",
      "Updated FF of Transformer Block 11 with weights and biases from incoming params.\n",
      "================================================\n",
      "All Transformer layers updated with incoming parameters.\n",
      "Updated Final Norm with scale and shift from incoming params.\n",
      "Updated Linear Out with weights and biases from incoming params.\n",
      "All weights updated with incoming parameters.\n"
     ]
    }
   ],
   "source": [
    "manual_weight_loader = ManualWeightLoading(custom_gpt2_model)\n",
    "manual_weight_loader.assign(gpt2_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Our Model Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialGPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm_1): LayerNorm()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (linear_out): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_gpt2_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Test output from assigned GPTModel instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21569,   318,   635,  1444,   355,   220]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['India is also called as \\xa0\"the most important country in the world\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "custom_gpt2_tokenizer = CUSTOM_GPT2Tokenizer()\n",
    "tokenized_in_strings = custom_gpt2_tokenizer.tokenize_batch(test_strings)\n",
    "print(tokenized_in_strings)\n",
    "\n",
    "# Model\n",
    "torch.manual_seed(seed_value)\n",
    "output_tokens = generate_text(custom_gpt2_model, tokenized_in_strings, max_tokens_to_generate=10, context_size=GPT2_124M_PARAMS[\"context_length\"], print_interims=False)\n",
    "\n",
    "# De-Tokenize\n",
    "custom_gpt2_tokenizer.detokenize_batch(output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "We know that we loaded the model weights correctly because the model can generate coherent text; if we made even a small mistake, the mode would not be able to do that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
